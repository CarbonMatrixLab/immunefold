defaults:
  - loss: abfold
  - model: esmfold
  - transforms: abfold

# device
gpu_list: [0]

# lora
lora:
 enabled: True
 lora_r_seq: 16
 lora_r_pair: 8
 lora_scaling: 1.0

model_align:
  lora: True
  bias: True
  fft_mode: 'only_evoformer'

# optimizer
num_epoch: 100000000000000000
warmup_steps: 0
flat_steps: 1000000000000000
decay_steps: 1000000000000000
learning_rate: 0.0005
min_lr: 5e-5

# model
restore_model_ckpt:  /home/zhutian/data/ab_data/esm2/ab_8000.ckpt
restore_esm2_model:  /home/zhutian/data/ab_data/esm2/esm2_t36_3B_UR50D.pt

is_ig_feature: True
shuffle_multimer_seq: True

# dataset
batch_size: 1
gradient_accumulation_it: 1
max_seq_len: 128
train_name_idx: /home/zhutian/data/ab_data/imfold_idx/train_cluster_new.idx
train_data: /home/zhutian/data/ab_data/ab_new_npz/
# train_name_idx: ../data_2023/pdb50_v2/clean_bc40_cluster.idx
# train_data: ../data_2023/pdb50_v2/data/
    
# output
output_dir: ./studies/abfold/2024-07-12

checkpoint_every_step: 500

verbose: 1
random_seed: 2023
