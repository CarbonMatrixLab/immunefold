defaults:
  - loss: abfold
  - model: esmfold
  - transforms: abfold

# device
gpu_list: [0,1,2,3]

# lora
lora:
 enabled: True
 lora_r_seq: 16
 lora_r_pair: 8
 lora_scaling: 1.0

model_align:
  lora: True
  bias: True
  fft_mode: 'only_evoformer'

# optimizer
num_epoch: 1024
warmup_steps: 0
flat_steps: 1000000000000000
decay_steps: 1000000000000000
learning_rate: 0.0001
min_lr: 5e-5

# model
restore_model_ckpt:  /home/zhutian/data/abdata_2023/esm2/carbonfold2_base_from_esmfold.ckpt
restore_esm2_model:  /home/zhutian/data/abdata_2023/esm2/step_20000_lora_merged.ckpt

is_ig_feature: True
shuffle_multimer_seq: True

# dataset
batch_size: 2
gradient_accumulation_it: 1
max_seq_len: 128
train_name_idx: /home/zhutian/data/antibody/ab_data/train_ab_cluster.idx
train_data: /home/zhutian/data/antibody/ab_npz/
# train_name_idx: ../data_2023/pdb50_v2/clean_bc40_cluster.idx
# train_data: ../data_2023/pdb50_v2/data/
    
# output
output_dir: ./studies/abfold/2024-05-11

checkpoint_every_step: 1000

verbose: 1
random_seed: 2023
